{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4cUyMMi8RygrWkOjCsZ5P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmZea2oFFVVM","executionInfo":{"status":"ok","timestamp":1714364657253,"user_tz":-330,"elapsed":9277,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"7290f452-c218-4489-9cf1-1456cfbbaa76"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k2RLBbK0Fg95","executionInfo":{"status":"ok","timestamp":1714364667564,"user_tz":-330,"elapsed":10315,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"b22a412f-1459-4445-8d8f-715d5c5da45d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"]}]},{"cell_type":"code","source":["!pip install scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16piYI4cVuuS","executionInfo":{"status":"ok","timestamp":1714364678800,"user_tz":-330,"elapsed":11249,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"d0578a79-878b-43c5-a004-b59b1f34eff1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"]}]},{"cell_type":"code","source":["corpus = ['the sky is blue', 'sky is blue and sky is beautiful', 'the beautiful sky is so blue', 'i love blue cheese']\n","new_doc = ['loving this blue sky today']"],"metadata":{"id":"Y6QdQrVmWLGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer"],"metadata":{"id":"ePh9STCDYtLc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BAG OF VECTOR MODEL - BOW\n","def bow_extractor(corpus, ngram_range=(1,1)):\n","  vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n","  features = vectorizer.fit_transform(corpus)\n","  return vectorizer,features"],"metadata":{"id":"QHjFNDIDWPy7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# build bow vectorizer and get features\n","# ROWS REPRESENT DOCUMENTS COLUMNS REPRESENT WORDS\n","bow_vectorizer,bow_features = bow_extractor(corpus)\n","features = bow_features.todense()\n","print(features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p4l-QYFnXCIV","executionInfo":{"status":"ok","timestamp":1714364680830,"user_tz":-330,"elapsed":7,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"2c1efe05-1735-4702-acf1-4aae50007b0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 1 0 1 0 1 0 1]\n"," [1 1 1 0 2 0 2 0 0]\n"," [0 1 1 0 1 0 1 1 1]\n"," [0 0 1 1 0 1 0 0 0]]\n"]}]},{"cell_type":"code","source":["# extract features from new document using built vectorizer\n","new_doc_features = bow_vectorizer.transform(new_doc)\n","new_doc_features = new_doc_features.todense()\n","print(new_doc_features)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nYK7FL56beBC","executionInfo":{"status":"ok","timestamp":1714364680830,"user_tz":-330,"elapsed":6,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"db1ca15b-8d4f-4a87-d367-72e5781df45c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 1 0 0 0 1 0 0]]\n"]}]},{"cell_type":"code","source":["# print the feature names\n","feature_names = bow_vectorizer.get_feature_names_out()\n","print(feature_names)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFwlWNuoWWII","executionInfo":{"status":"ok","timestamp":1714364680830,"user_tz":-330,"elapsed":4,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"4db783dd-1a0d-494d-9e47-e5619b9c4345"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['and' 'beautiful' 'blue' 'cheese' 'is' 'love' 'sky' 'so' 'the']\n"]}]},{"cell_type":"code","source":["# Understanding the feature vectorization with detailed representation\n","import pandas as pd\n","def display_features(features, feature_names):\n","  df = pd.DataFrame(data=features,columns=feature_names)\n","  print(df)\n","display_features(features, feature_names)\n","display_features(new_doc_features, feature_names)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhNnPrsZbyZI","executionInfo":{"status":"ok","timestamp":1714364681476,"user_tz":-330,"elapsed":649,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"1a1c6b87-fef9-4d2c-b999-aa6475056c32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   and  beautiful  blue  cheese  is  love  sky  so  the\n","0    0          0     1       0   1     0    1   0    1\n","1    1          1     1       0   2     0    2   0    0\n","2    0          1     1       0   1     0    1   1    1\n","3    0          0     1       1   0     1    0   0    0\n","   and  beautiful  blue  cheese  is  love  sky  so  the\n","0    0          0     1       0   0     0    1   0    0\n"]}]},{"cell_type":"code","source":["# TF - DF MODEL\n","from sklearn.feature_extraction.text import TfidfTransformer\n","def tfidf_transformer(bow_matrix):\n","    transformer = TfidfTransformer(norm='l2',smooth_idf=True,use_idf=True)\n","    tfidf_matrix = transformer.fit_transform(bow_matrix)\n","    return transformer, tfidf_matrix"],"metadata":{"id":"b0cxBN4UdD_4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfTransformer\n","feature_names = bow_vectorizer.get_feature_names_out()"],"metadata":{"id":"1rv6f147db7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfTransformer\n","def tfidf_transformer(bow_matrix):\n","  transformer = TfidfTransformer(norm='l2',smooth_idf=True,use_idf=True)\n","  tfidf_matrix = transformer.fit_transform(bow_matrix)\n","  return transformer, tfidf_matrix"],"metadata":{"id":"xf3GetYjcBwn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.feature_extraction.text import TfidfTransformer\n","feature_names = bow_vectorizer.get_feature_names_out()"],"metadata":{"id":"q40f1QJDeMtf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# build tfidf transformer and show train corpus tfidf features\n","tfidf_trans, tdidf_features = tfidf_transformer(bow_features)\n","features = np.round(tdidf_features.todense(), 2)\n","display_features(features, feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kv1xCX_AeE2Q","executionInfo":{"status":"ok","timestamp":1714364681477,"user_tz":-330,"elapsed":21,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"00250d77-9ead-42b3-f835-d1081ae75952"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    and  beautiful  blue  cheese    is  love   sky    so   the\n","0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n","1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n","2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n","3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"]}]},{"cell_type":"code","source":["# show tfidf features for new_doc using built tfidf transformer\n","new_doc_features = np.asarray(new_doc_features)\n","nd_tfidf = tfidf_trans.transform(new_doc_features)\n","nd_features = np.round(nd_tfidf.todense(), 2)\n","display_features(nd_features, feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpjdiIvCe7ef","executionInfo":{"status":"ok","timestamp":1714364681477,"user_tz":-330,"elapsed":20,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"ac233383-4d01-49cd-9304-b5ba85cccb77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   and  beautiful  blue  cheese   is  love   sky   so  the\n","0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"]}]},{"cell_type":"code","source":["# Understanding how it works in the background\n","import scipy.sparse as sp\n","from numpy.linalg import norm\n","feature_names = bow_vectorizer.get_feature_names_out()\n","# compute term frequency\n","tf = bow_features.todense()\n","tf = np.array(tf, dtype='float64')\n","# show term frequencies\n","display_features(tf, feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J751JYmJfgcB","executionInfo":{"status":"ok","timestamp":1714364681477,"user_tz":-330,"elapsed":19,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"a6440ed6-91cb-4e92-eb1c-481df752a80f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   and  beautiful  blue  cheese   is  love  sky   so  the\n","0  0.0        0.0   1.0     0.0  1.0   0.0  1.0  0.0  1.0\n","1  1.0        1.0   1.0     0.0  2.0   0.0  2.0  0.0  0.0\n","2  0.0        1.0   1.0     0.0  1.0   0.0  1.0  1.0  1.0\n","3  0.0        0.0   1.0     1.0  0.0   1.0  0.0  0.0  0.0\n"]}]},{"cell_type":"code","source":["# show tfidf features for new_doc using built tfidf transformer\n","nd_tfidf = tfidf_trans.transform(new_doc_features)\n","nd_features = np.round(nd_tfidf.todense(), 2)\n","display_features(nd_features, feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uamug8eGfz5-","executionInfo":{"status":"ok","timestamp":1714364681477,"user_tz":-330,"elapsed":17,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"67dff69c-5b27-429f-b610-da5c1895e4a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   and  beautiful  blue  cheese   is  love   sky   so  the\n","0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"]}]},{"cell_type":"code","source":["# build the document frequency matrix\n","df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n","df = 1 + df # to smoothen idf later\n","# show document frequencies\n","display_features([df], feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0psl3whf71N","executionInfo":{"status":"ok","timestamp":1714364681477,"user_tz":-330,"elapsed":16,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"635a733a-1ffa-4b22-9b48-af713101b787"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   and  beautiful  blue  cheese  is  love  sky  so  the\n","0    2          3     5       2   4     2    4   2    3\n"]}]},{"cell_type":"code","source":["# compute inverse document frequencies\n","total_docs = 1 + len(corpus)\n","idf = 1.0 + np.log(float(total_docs) / df)\n","# show inverse document frequencies\n","display_features([np.round(idf, 2)], feature_names)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iBRbwG3Tf-m8","executionInfo":{"status":"ok","timestamp":1714364681478,"user_tz":-330,"elapsed":16,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"45b8a2e0-1fb9-492f-d4f0-088fa9924012"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    and  beautiful  blue  cheese    is  love   sky    so   the\n","0  1.92       1.51   1.0    1.92  1.22  1.92  1.22  1.92  1.51\n"]}]},{"cell_type":"code","source":["# compute idf diagonal matrix\n","total_features = bow_features.shape[1]\n","idf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\n","idf = idf_diag.todense()\n","# print the idf diagonal matrix\n","print(np.round(idf, 2))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U3003ujOgKEp","executionInfo":{"status":"ok","timestamp":1714364681478,"user_tz":-330,"elapsed":14,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"54f9a0c9-b232-4b15-b0bc-5841167b4f20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.92 0.   0.   0.   0.   0.   0.   0.   0.  ]\n"," [0.   1.51 0.   0.   0.   0.   0.   0.   0.  ]\n"," [0.   0.   1.   0.   0.   0.   0.   0.   0.  ]\n"," [0.   0.   0.   1.92 0.   0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.   1.22 0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.   0.   1.92 0.   0.   0.  ]\n"," [0.   0.   0.   0.   0.   0.   1.22 0.   0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.   1.92 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.   0.   1.51]]\n"]}]},{"cell_type":"code","source":["# Compute the tf idf feature matrix using matrix multiplication.\n","tfidf = tf * idf\n","# show tfidf feature matrix\n","display_features(np.round(tfidf, 2), feature_names)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oScmH8x4gRcl","executionInfo":{"status":"ok","timestamp":1714364681479,"user_tz":-330,"elapsed":14,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"5bf83cda-96d2-43a3-d943-3b84e8ff00a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    and  beautiful  blue  cheese    is  love   sky    so   the\n","0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  0.00  1.51\n","1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00  0.00\n","2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.92  1.51\n","3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00  0.00\n"]}]},{"cell_type":"markdown","source":["#Computes the tfidf norms for each document and then divides the tfidf weights with the norm as per the formula to give us the final desired tfidf matrix"],"metadata":{"id":"xz0TzjC0NMPx"}},{"cell_type":"code","source":["# compute L2 norms\n","norms = norm(tfidf, axis=1)\n","# print norms for each document\n","print(np.round(norms, 2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkkQG-5oM0ZR","executionInfo":{"status":"ok","timestamp":1714364681480,"user_tz":-330,"elapsed":14,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"ec6f0a48-cdb7-44d5-cd22-32d277f07d3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2.5  4.35 3.5  2.89]\n"]}]},{"cell_type":"code","source":["# compute normalized tfidf\n","norm_tfidf = tfidf / norms[:, None]\n","# show final tfidf feature matrix\n","display_features(np.round(norm_tfidf, 2), feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uT4w7UvtM6FN","executionInfo":{"status":"ok","timestamp":1714364681480,"user_tz":-330,"elapsed":13,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"5f39639b-ded4-4fe7-a1c4-fce4edc777af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    and  beautiful  blue  cheese    is  love   sky    so   the\n","0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n","1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n","2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n","3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"]}]},{"cell_type":"markdown","source":["# Compare tfidf feature matrix for the documents in CORPUS to the feature matrix obtained using TfidfTransformer earlier. They are same, which means the mathematical formula appied to compute is correct."],"metadata":{"id":"26BsTBgIOFRv"}},{"cell_type":"code","source":["# compute the tfidfbased feature matrix for new document new_doc.\n","# compute new doc term freqs from bow freqs\n","nd_tf = new_doc_features\n","nd_tf = np.array(nd_tf, dtype='float64')"],"metadata":{"id":"MxMlQvjbOjS0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compute tfidf using idf matrix from train corpus\n","nd_tfidf = nd_tf*idf\n","nd_norms = norm(nd_tfidf, axis=1)\n","norm_nd_tfidf = nd_tfidf / nd_norms[:, None]"],"metadata":{"id":"o9NcU4OhOwEt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show new_doc tfidf feature vector\n","display_features(np.round(norm_nd_tfidf, 2), feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wr_dykQMOy-G","executionInfo":{"status":"ok","timestamp":1714364681480,"user_tz":-330,"elapsed":12,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"e549d7c2-17d5-48b7-c350-e3c9eff43a03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   and  beautiful  blue  cheese   is  love   sky   so  the\n","0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"]}]},{"cell_type":"markdown","source":["# Observe again this matrix is same as earlier.\n","# We now implement a generic function that can directly compute the tfidf-based feature vectors for documents from the raw documents themselves.\n","# This generic function makes use of the TfidfVectorizer, which directly computes the tfidf vectors by taking the raw documents themselves as input and internally computing the term frequencies as well as the inverse document frequencies, eliminating the need to use the CountVectorizer for computing the term frequencies based on the Bag of Words model."],"metadata":{"id":"1u5iKJDlPYtm"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","def tfidf_extractor(corpus, ngram_range=(1,1)):\n","  vectorizer = TfidfVectorizer(min_df=1,norm='l2',smooth_idf=True,use_idf=True,ngram_range=ngram_range)\n","  features = vectorizer.fit_transform(corpus)\n","  return vectorizer, features"],"metadata":{"id":"jdNX_C8xP13Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# build tfidf vectorizer and get training corpus feature vectors\n","tfidf_vectorizer, tdidf_features = tfidf_extractor(corpus)\n","display_features(np.round(tdidf_features.todense(), 2),feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X71s8EIYQdin","executionInfo":{"status":"ok","timestamp":1714364681480,"user_tz":-330,"elapsed":10,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"49d92f08-2fbb-4e71-edc4-448d120afe2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    and  beautiful  blue  cheese    is  love   sky    so   the\n","0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n","1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n","2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n","3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"]}]},{"cell_type":"markdown","source":["# Observe that it is again same as above for corpus as well as new_doc"],"metadata":{"id":"PKggReLuQ00-"}},{"cell_type":"code","source":["# get tfidf feature vector for the new document\n","nd_tfidf = tfidf_vectorizer.transform(new_doc)\n","display_features(np.round(nd_tfidf.todense(), 2), feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yXLg10TVQxSW","executionInfo":{"status":"ok","timestamp":1714364682021,"user_tz":-330,"elapsed":550,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"9c297070-128c-4bcb-9b8d-b4ce74ce169f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   and  beautiful  blue  cheese   is  love   sky   so  the\n","0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"]}]},{"cell_type":"markdown","source":["# Modern Word Vector Models - Advanced Word Vectorization Models :"],"metadata":{"id":"sbstAd40DOFG"}},{"cell_type":"code","source":["# Word 2 Vector Model\n","import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kUYbkhZhB7j","executionInfo":{"status":"ok","timestamp":1714364683411,"user_tz":-330,"elapsed":1392,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"48325ba3-a279-4bd7-cff5-5ab3c306133b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["import gensim"],"metadata":{"id":"WKe2O2bCgWTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Recall\n","corpus = ['the sky is blue', 'sky is blue and sky is beautiful', 'the beautiful sky is so blue', 'i love blue cheese']\n","new_doc = ['loving this blue sky today']"],"metadata":{"id":"jkuc9swcUNOT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5zfGyzvTFylm","executionInfo":{"status":"ok","timestamp":1714364684147,"user_tz":-330,"elapsed":6,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"f99b14fa-550c-4a70-ada8-80040b4cad27"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the sky is blue',\n"," 'sky is blue and sky is beautiful',\n"," 'the beautiful sky is so blue',\n"," 'i love blue cheese']"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["new_doc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sIoT7tmZFrOq","executionInfo":{"status":"ok","timestamp":1714364684147,"user_tz":-330,"elapsed":5,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"b173ff18-9bed-4490-9fb6-7251e57bb812"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['loving this blue sky today']"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["# tokenize corpora\n","TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) for sentence in corpus]\n","print(TOKENIZED_CORPUS)\n","tokenized_new_doc = [nltk.word_tokenize(sentence) for sentence in new_doc]\n","print(tokenized_new_doc)"],"metadata":{"id":"7EDz-JbQhSWY","executionInfo":{"status":"ok","timestamp":1714364684809,"user_tz":-330,"elapsed":665,"user":{"displayName":"professor","userId":"02563211122263964727"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"38873886-2df1-4836-8714-2d70139b73b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['the', 'sky', 'is', 'blue'], ['sky', 'is', 'blue', 'and', 'sky', 'is', 'beautiful'], ['the', 'beautiful', 'sky', 'is', 'so', 'blue'], ['i', 'love', 'blue', 'cheese']]\n","[['loving', 'this', 'blue', 'sky', 'today']]\n"]}]},{"cell_type":"code","source":["# build the word2vec model on our training corpus\n","model = gensim.models.Word2Vec(TOKENIZED_CORPUS,vector_size=10,window=10,min_count=2,sample=1e-3)"],"metadata":{"id":"UswehIDiZLiG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model creates a vector representation for each word in the vocabulary\n","# Check whether model of Word2Vector type is been created\n","print(type(model))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Zg0WbTPU4sJ","executionInfo":{"status":"ok","timestamp":1714364684810,"user_tz":-330,"elapsed":16,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"dba3130a-86fd-4233-c73a-f4c374fb704c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'gensim.models.word2vec.Word2Vec'>\n"]}]},{"cell_type":"code","source":["# The Word2Vec model stores word vectors in a separate attribute called wv.\n","# It is a dictionary\n","print(model.wv.key_to_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AmVoWIv-ilUf","executionInfo":{"status":"ok","timestamp":1714364684810,"user_tz":-330,"elapsed":15,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"f69443ee-ee3f-43ea-88d8-bdcebf516519"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'blue': 0, 'is': 1, 'sky': 2, 'beautiful': 3, 'the': 4}\n"]}]},{"cell_type":"code","source":["word = \"sky\"\n","vector = model.wv[word]\n","print(vector)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17aniugihvIB","executionInfo":{"status":"ok","timestamp":1714364684810,"user_tz":-330,"elapsed":14,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"5432b4bc-43e9-44cf-8324-a3ffc67b8bca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 0.07311766  0.05070262  0.06757693  0.00762866  0.06350891 -0.03405366\n"," -0.00946401  0.05768573 -0.07521638 -0.03936104]\n"]}]},{"cell_type":"code","source":["word = \"blue\"\n","vector = model.wv[word]\n","print(vector)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o38hRU-pi0PT","executionInfo":{"status":"ok","timestamp":1714364684810,"user_tz":-330,"elapsed":13,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"e9260144-0334-4c20-e72d-3cbc039d9f8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.00536227  0.00236431  0.0510335   0.09009273 -0.0930295  -0.07116809\n","  0.06458873  0.08972988 -0.05015428 -0.03763372]\n"]}]},{"cell_type":"markdown","source":["# Add all the word vectors and divide the result by the total number of words matched in the vocabulary to get a final resulting averaged word vector representation for the text document."],"metadata":{"id":"tsgVxMPcWqUZ"}},{"cell_type":"code","source":["import numpy as np\n","# define function to average word vectors for a text document\n","def average_word_vectors(words, model, vocabulary, num_features):\n","  feature_vector = np.zeros((num_features,),dtype=\"float64\")\n","  nwords = 0\n","  for word in words:\n","    if word in vocabulary:\n","      nwords = nwords + 1\n","      feature_vector = np.add(feature_vector, model.wv[word])\n","    if nwords:\n","      feature_vector = np.divide(feature_vector,nwords)\n","  return feature_vector"],"metadata":{"id":"1dwcFsofjUbH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generalize above function for a corpus of documents\n","def averaged_word_vectorizer(corpus, model, num_features):\n","  vocabulary = set(model.wv.index_to_key)\n","  features = [average_word_vectors(tokenized_sentence, model, vocabulary,num_features)for tokenized_sentence in corpus]\n","  return np.array(features)"],"metadata":{"id":"xzRpjYWSjp7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from gensim.models import Word2Vec"],"metadata":{"id":"OjALL2B9XijG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get averaged word vectors for our training CORPUS\n","avg_word_vec_features = averaged_word_vectorizer(corpus=TOKENIZED_CORPUS, model=model, num_features=10)\n","print(np.round(avg_word_vec_features, 3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uimT1dRBadu9","executionInfo":{"status":"ok","timestamp":1714364684811,"user_tz":-330,"elapsed":9,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"a294ca75-14c0-42c0-d040-90559bfea6d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.004  0.004  0.008  0.026 -0.025 -0.021  0.015  0.03  -0.021 -0.015]\n"," [-0.009 -0.002  0.015 -0.01  -0.005 -0.004  0.014 -0.009 -0.003 -0.011]\n"," [-0.     0.001  0.01   0.019 -0.019 -0.015  0.013  0.018 -0.011 -0.009]\n"," [-0.005  0.002  0.051  0.09  -0.093 -0.071  0.065  0.09  -0.05  -0.038]]\n"]}]},{"cell_type":"code","source":["# Get averaged word vectors for our training CORPUS\n","avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_new_doc, model=model, num_features=10)\n","print(np.round(avg_word_vec_features, 3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNnsvcz0fkoB","executionInfo":{"status":"ok","timestamp":1714364684811,"user_tz":-330,"elapsed":8,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"10d50d7a-2a5b-458e-a682-9d5f33974a21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.017  0.013  0.03   0.024 -0.007 -0.026  0.014  0.037 -0.031 -0.019]]\n"]}]},{"cell_type":"markdown","source":["# From the above outputs, we can see that we have uniformly sized averaged word vectors for each document in the corpus, and these feature vectors will be used later for classification by feeding it to the ML algorithms"],"metadata":{"id":"EwUI9piZf-Dc"}}]}