{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"JpLS0vX-UYQ_","executionInfo":{"status":"ok","timestamp":1717923129564,"user_tz":-330,"elapsed":1587,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["import nltk"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1177,"status":"ok","timestamp":1717923130737,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"YXcrjbIkXe92","outputId":"784e4bf9-ecae-4fa9-bd24-f247795ff576"},"outputs":[{"output_type":"stream","name":"stdout","text":["b': -4em;\\r\\n    margin-left: 4em;\\r\\n    margin-top: 1em;\\r\\n    margin-bottom: 0;\\r\\n    font-size: medium\\r\\n}\\r\\n#pg-header #pg-header-authlist {\\r\\n    all: initial;\\r\\n    margin-top: 0;\\r\\n    margin-bottom: 0;\\r\\n}\\r\\n#pg-header #pg-machine-header strong {\\r\\n    font-weight: normal;\\r\\n}\\r\\n#pg-header #pg-start-separator, #pg-footer #pg-end-separator {\\r\\n    margin-bottom: 3em;\\r\\n    margin-left: 0;\\r\\n    margin-right: auto;\\r\\n    margin-top: 2em;\\r\\n    text-align: center\\r\\n}\\r\\n\\r\\n    .xhtml_center {text-align: center; display: block;}\\r\\n    .xhtml_center table {\\r\\n        display: table;\\r\\n        text-align: left;\\r\\n        margin-left: auto;\\r\\n        margin-right: auto;\\r\\n        }</style><title>The Project Gutenberg eBook of The Bible, King James version, Book 1: Genesis, by Anonymous</title><style>/* ************************************************************************\\r\\n * classless css copied from https://www.pgdp.net/wiki/CSS_Cookbook/Styles\\r\\n * ********************************************************************** */\\r\\n/* ************************************************************************\\r\\n * set the body margins to allow whitespace along sides of window\\r\\n * ******************************************'\n"]}],"source":["import requests\n","data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n","content = data.content\n","print(content[1000:2200])"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"txZz7Ov3Xp93","executionInfo":{"status":"ok","timestamp":1717923130738,"user_tz":-330,"elapsed":11,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["# 1_Removign HTML Tags. Import re\n","import re"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"tbkwZChhXr-D","executionInfo":{"status":"ok","timestamp":1717923130738,"user_tz":-330,"elapsed":9,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["from bs4 import BeautifulSoup"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"VFVLcAi0XvDh","executionInfo":{"status":"ok","timestamp":1717923130739,"user_tz":-330,"elapsed":9,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["def html_stripping(text):\n","  soup = BeautifulSoup(text, \"html.parser\")\n","  [s.extract() for s in soup(['iframe', 'script'])]\n","  stripped_text = soup.get_text()\n","  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n","  return stripped_text"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":969,"status":"ok","timestamp":1717923131700,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"IZzku6XSX13l","outputId":"fa09f9a3-eb19-4c34-d408-2ff3b216ee7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Book 01        Genesis\n","01:001:001 In the beginning God created the heaven and the earth.\n","01:001:002 And the earth was without form, and void; and darkness was\n","           upon the face of the deep. And the Spirit of God moved upon\n","           the face of the waters.\n","01:001:003 And God said, Let there be light: and there was light.\n","01:001:004 And God saw the light, that it was good: and God divided the\n","           light from the darkness.\n","01:001:005 And God called the light Day, and the darkness he called\n","           Night. And the evening and the morning were the first day.\n","01:001:006 And God said, Let there be a firmament in the midst of the\n","           waters, and let it divide the waters from the waters.\n","01:001:007 And God made the firmament, and divided the waters which were\n","           under the firmament from the waters which were above the\n","           firmament: and it was so.\n","01:001:008 And God called the firmament Heaven. And the evening and the\n","           morning were the second day.\n","01:001\n"]}],"source":["clean_content = html_stripping(content)\n","print(clean_content[1036:2045])"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"MYS-xvoJX4V6","executionInfo":{"status":"ok","timestamp":1717923131700,"user_tz":-330,"elapsed":7,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["import nltk\n","from nltk.corpus import gutenberg\n","from pprint import pprint\n","import numpy as np"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1nEE2rkFYE80","outputId":"c52f5f22-2c6c-43cc-91c2-c138cd544c80","executionInfo":{"status":"ok","timestamp":1717923186939,"user_tz":-330,"elapsed":55244,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}],"source":["nltk.download('all')"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1717923186940,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"I9qtzlaKfKDK","outputId":"fee66cef-f197-46fb-9b2b-bfc037f37c67"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["nltk.download('punkt')\n","import re\n","import string\n","from pprint import pprint"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"yM2r-aFzUo0x","executionInfo":{"status":"ok","timestamp":1717923186940,"user_tz":-330,"elapsed":18,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["# Creating our own corpus\n","corpus=[\"The brown fox wasn't quick and he couldn't win the race.\",\n","\"Hey it was a great cricket match @yesterday!!\",\n","\"I just bought a @new mobile for me at $1000.\",\n","\"Python NLP is really ****amazing*****!!@@@\"]"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"pN-datxmpgeB","executionInfo":{"status":"ok","timestamp":1717923186941,"user_tz":-330,"elapsed":18,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["def tokenize_text(text):\n","  sentences=nltk.sent_tokenize(text)\n","  print(sentences)\n","  word_tokens=[nltk.word_tokenize(sentence) for sentence in sentences]\n","  return word_tokens"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1717923186941,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"RmNp78CMpl-T","outputId":"5898b505-0a5b-4d3d-a5c7-97a3e381e072"},"outputs":[{"output_type":"stream","name":"stdout","text":["[\"The brown fox wasn't quick and he couldn't win the race.\"]\n","['Hey it was a great cricket match @yesterday!', '!']\n","['I just bought a @new mobile for me at $1000.']\n","['Python NLP is really ****amazing*****!!', '@@@']\n","[[['The', 'brown', 'fox', 'was', \"n't\", 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race', '.']], [['Hey', 'it', 'was', 'a', 'great', 'cricket', 'match', '@', 'yesterday', '!'], ['!']], [['I', 'just', 'bought', 'a', '@', 'new', 'mobile', 'for', 'me', 'at', '$', '1000', '.']], [['Python', 'NLP', 'is', 'really', '*', '*', '*', '*', 'amazing', '*', '*', '*', '*', '*', '!', '!'], ['@', '@', '@']]]\n"]}],"source":["# Get the token list from the above def\n","token_list=[tokenize_text(text) for text in corpus]\n","print(token_list)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"e2clNxxTZA7d","executionInfo":{"status":"ok","timestamp":1717923186941,"user_tz":-330,"elapsed":14,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["# 2 - Removing special characters\n","def remove_special_characters(sentence,keep_apostrophes=False):\n","  sentence = sentence.strip()\n","  if keep_apostrophes:\n","    PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n","    filtered_sentence = re.sub(PATTERN, r'', sentence)\n","  else:\n","    PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n","    filtered_sentence = re.sub(PATTERN, r'', sentence)\n","  return filtered_sentence"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"g9D5mNkhr3LN","executionInfo":{"status":"ok","timestamp":1717923186941,"user_tz":-330,"elapsed":14,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["sentence=\"The brown fox wasn't quick and he couldn't win the race.\""]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1717923186941,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"vmqq49HQoM-a","outputId":"2238cf7e-d79c-4d36-ccac-3755025b97ce"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The brown fox wasnt quick and he couldnt win the race'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["remove_special_characters(sentence,keep_apostrophes=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6979,"status":"ok","timestamp":1717923193909,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"PwJprYHEZlNu","outputId":"b5833755-2dff-436e-ff89-ac8ca741bad3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Collecting textsearch>=0.0.21 (from contractions)\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Collecting anyascii (from textsearch>=0.0.21->contractions)\n","  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n","  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"]}],"source":["!pip install contractions"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"hmr81PCkZp8z","executionInfo":{"status":"ok","timestamp":1717923193909,"user_tz":-330,"elapsed":11,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["import contractions"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1717923193909,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"SJNuqzySZtWj","outputId":"5c5b81e4-b154-4b80-e2a4-039414d80f7b"},"outputs":[{"output_type":"stream","name":"stdout","text":["{\"I'm\": 'I am', \"I'm'a\": 'I am about to', \"I'm'o\": 'I am going to', \"I've\": 'I have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'd\": 'I would', \"I'd've\": 'I would have', 'Whatcha': 'What are you', \"amn't\": 'am not', \"ain't\": 'are not', \"aren't\": 'are not', \"'cause\": 'because', \"can't\": 'cannot', \"can't've\": 'cannot have', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"daren't\": 'dare not', \"daresn't\": 'dare not', \"dasn't\": 'dare not', \"didn't\": 'did not', 'didn’t': 'did not', \"don't\": 'do not', 'don’t': 'do not', \"doesn't\": 'does not', \"e'er\": 'ever', \"everyone's\": 'everyone is', 'finna': 'fixing to', 'gimme': 'give me', \"gon't\": 'go not', 'gonna': 'going to', 'gotta': 'got to', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he've\": 'he have', \"he's\": 'he is', \"he'll\": 'he will', \"he'll've\": 'he will have', \"he'd\": 'he would', \"he'd've\": 'he would have', \"here's\": 'here is', \"how're\": 'how are', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how's\": 'how is', \"how'll\": 'how will', \"isn't\": 'is not', \"it's\": 'it is', \"'tis\": 'it is', \"'twas\": 'it was', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it'd\": 'it would', \"it'd've\": 'it would have', 'kinda': 'kind of', \"let's\": 'let us', 'luv': 'love', \"ma'am\": 'madam', \"may've\": 'may have', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"ne'er\": 'never', \"o'\": 'of', \"o'clock\": 'of the clock', \"ol'\": 'old', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"o'er\": 'over', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shalln't\": 'shall not', \"shan't've\": 'shall not have', \"she's\": 'she is', \"she'll\": 'she will', \"she'd\": 'she would', \"she'd've\": 'she would have', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so is', \"somebody's\": 'somebody is', \"someone's\": 'someone is', \"something's\": 'something is', 'sux': 'sucks', \"that're\": 'that are', \"that's\": 'that is', \"that'll\": 'that will', \"that'd\": 'that would', \"that'd've\": 'that would have', \"'em\": 'them', \"there're\": 'there are', \"there's\": 'there is', \"there'll\": 'there will', \"there'd\": 'there would', \"there'd've\": 'there would have', \"these're\": 'these are', \"they're\": 'they are', \"they've\": 'they have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they'd\": 'they would', \"they'd've\": 'they would have', \"this's\": 'this is', \"this'll\": 'this will', \"this'd\": 'this would', \"those're\": 'those are', \"to've\": 'to have', 'wanna': 'want to', \"wasn't\": 'was not', \"we're\": 'we are', \"we've\": 'we have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we'd\": 'we would', \"we'd've\": 'we would have', \"weren't\": 'were not', \"what're\": 'what are', \"what'd\": 'what did', \"what've\": 'what have', \"what's\": 'what is', \"what'll\": 'what will', \"what'll've\": 'what will have', \"when've\": 'when have', \"when's\": 'when is', \"where're\": 'where are', \"where'd\": 'where did', \"where've\": 'where have', \"where's\": 'where is', \"which's\": 'which is', \"who're\": 'who are', \"who've\": 'who have', \"who's\": 'who is', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who'd\": 'who would', \"who'd've\": 'who would have', \"why're\": 'why are', \"why'd\": 'why did', \"why've\": 'why have', \"why's\": 'why is', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"you're\": 'you are', \"you've\": 'you have', \"you'll've\": 'you shall have', \"you'll\": 'you will', \"you'd\": 'you would', \"you'd've\": 'you would have', 'to cause': 'to cause', 'will cause': 'will cause', 'should cause': 'should cause', 'would cause': 'would cause', 'can cause': 'can cause', 'could cause': 'could cause', 'must cause': 'must cause', 'might cause': 'might cause', 'shall cause': 'shall cause', 'may cause': 'may cause', 'jan.': 'january', 'feb.': 'february', 'mar.': 'march', 'apr.': 'april', 'jun.': 'june', 'jul.': 'july', 'aug.': 'august', 'sep.': 'september', 'oct.': 'october', 'nov.': 'november', 'dec.': 'december', 'I’m': 'I am', 'I’m’a': 'I am about to', 'I’m’o': 'I am going to', 'I’ve': 'I have', 'I’ll': 'I will', 'I’ll’ve': 'I will have', 'I’d': 'I would', 'I’d’ve': 'I would have', 'amn’t': 'am not', 'ain’t': 'are not', 'aren’t': 'are not', '’cause': 'because', 'can’t': 'cannot', 'can’t’ve': 'cannot have', 'could’ve': 'could have', 'couldn’t': 'could not', 'couldn’t’ve': 'could not have', 'daren’t': 'dare not', 'daresn’t': 'dare not', 'dasn’t': 'dare not', 'doesn’t': 'does not', 'e’er': 'ever', 'everyone’s': 'everyone is', 'gon’t': 'go not', 'hadn’t': 'had not', 'hadn’t’ve': 'had not have', 'hasn’t': 'has not', 'haven’t': 'have not', 'he’ve': 'he have', 'he’s': 'he is', 'he’ll': 'he will', 'he’ll’ve': 'he will have', 'he’d': 'he would', 'he’d’ve': 'he would have', 'here’s': 'here is', 'how’re': 'how are', 'how’d': 'how did', 'how’d’y': 'how do you', 'how’s': 'how is', 'how’ll': 'how will', 'isn’t': 'is not', 'it’s': 'it is', '’tis': 'it is', '’twas': 'it was', 'it’ll': 'it will', 'it’ll’ve': 'it will have', 'it’d': 'it would', 'it’d’ve': 'it would have', 'let’s': 'let us', 'ma’am': 'madam', 'may’ve': 'may have', 'mayn’t': 'may not', 'might’ve': 'might have', 'mightn’t': 'might not', 'mightn’t’ve': 'might not have', 'must’ve': 'must have', 'mustn’t': 'must not', 'mustn’t’ve': 'must not have', 'needn’t': 'need not', 'needn’t’ve': 'need not have', 'ne’er': 'never', 'o’': 'of', 'o’clock': 'of the clock', 'ol’': 'old', 'oughtn’t': 'ought not', 'oughtn’t’ve': 'ought not have', 'o’er': 'over', 'shan’t': 'shall not', 'sha’n’t': 'shall not', 'shalln’t': 'shall not', 'shan’t’ve': 'shall not have', 'she’s': 'she is', 'she’ll': 'she will', 'she’d': 'she would', 'she’d’ve': 'she would have', 'should’ve': 'should have', 'shouldn’t': 'should not', 'shouldn’t’ve': 'should not have', 'so’ve': 'so have', 'so’s': 'so is', 'somebody’s': 'somebody is', 'someone’s': 'someone is', 'something’s': 'something is', 'that’re': 'that are', 'that’s': 'that is', 'that’ll': 'that will', 'that’d': 'that would', 'that’d’ve': 'that would have', '’em': 'them', 'there’re': 'there are', 'there’s': 'there is', 'there’ll': 'there will', 'there’d': 'there would', 'there’d’ve': 'there would have', 'these’re': 'these are', 'they’re': 'they are', 'they’ve': 'they have', 'they’ll': 'they will', 'they’ll’ve': 'they will have', 'they’d': 'they would', 'they’d’ve': 'they would have', 'this’s': 'this is', 'this’ll': 'this will', 'this’d': 'this would', 'those’re': 'those are', 'to’ve': 'to have', 'wasn’t': 'was not', 'we’re': 'we are', 'we’ve': 'we have', 'we’ll': 'we will', 'we’ll’ve': 'we will have', 'we’d': 'we would', 'we’d’ve': 'we would have', 'weren’t': 'were not', 'what’re': 'what are', 'what’d': 'what did', 'what’ve': 'what have', 'what’s': 'what is', 'what’ll': 'what will', 'what’ll’ve': 'what will have', 'when’ve': 'when have', 'when’s': 'when is', 'where’re': 'where are', 'where’d': 'where did', 'where’ve': 'where have', 'where’s': 'where is', 'which’s': 'which is', 'who’re': 'who are', 'who’ve': 'who have', 'who’s': 'who is', 'who’ll': 'who will', 'who’ll’ve': 'who will have', 'who’d': 'who would', 'who’d’ve': 'who would have', 'why’re': 'why are', 'why’d': 'why did', 'why’ve': 'why have', 'why’s': 'why is', 'will’ve': 'will have', 'won’t': 'will not', 'won’t’ve': 'will not have', 'would’ve': 'would have', 'wouldn’t': 'would not', 'wouldn’t’ve': 'would not have', 'y’all': 'you all', 'y’all’re': 'you all are', 'y’all’ve': 'you all have', 'y’all’d': 'you all would', 'y’all’d’ve': 'you all would have', 'you’re': 'you are', 'you’ve': 'you have', 'you’ll’ve': 'you shall have', 'you’ll': 'you will', 'you’d': 'you would', 'you’d’ve': 'you would have'}\n"]}],"source":["contraction_mapping=contractions.contractions_dict\n","print(contraction_mapping)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"Ua7EsEXlZzq0","executionInfo":{"status":"ok","timestamp":1717923193909,"user_tz":-330,"elapsed":6,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["# 3_Replacing contractions with expanded form\n","def contraction_expansion(sentence,contraction_mapping):\n","  contractions_pattern=re.compile('({})'.format('|'.join(contraction_mapping.keys())),flags=re.IGNORECASE|re.DOTALL)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"C8XZSoyhcFcx","executionInfo":{"status":"ok","timestamp":1717923194863,"user_tz":-330,"elapsed":959,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["# To remove the accented characters\n","import unicodedata"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"Y_1fdW76cIXi","executionInfo":{"status":"ok","timestamp":1717923194864,"user_tz":-330,"elapsed":23,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["# def for removing accented characters\n","def accented_char_removal(text):\n","  text=str(text)\n","  text = unicodedata.normalize('NFKD',text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","  return text"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1717923194864,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"RKcTwbxqglKm","outputId":"400ef041-dbcd-4822-c458-3576f053af91"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Some Accented text'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}],"source":["accented_char_removal('Sómě Áccěntěd těxt')"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"FNdqndd4cVca","executionInfo":{"status":"ok","timestamp":1717923194864,"user_tz":-330,"elapsed":20,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["# 4 _ Special Character Removal\n","def special_char_removal(text, remove_digits=False):\n","  pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n","  text = re.sub(pattern, '', text)\n","  return text"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1717923194864,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"54-bsBQ4csCt","outputId":"b5f0ce30-c088-4c2f-8299-076cc506d981"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Well this was fun What do you think '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}],"source":["special_char_removal(\"Well this was fun! What do you think? 123#@!\",remove_digits=True)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"_9at4lqedDh_","executionInfo":{"status":"ok","timestamp":1717923194864,"user_tz":-330,"elapsed":18,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["# 4_ Stemming\n","from nltk.stem import PorterStemmer"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"OtHzyHZ3dE_p","executionInfo":{"status":"ok","timestamp":1717923194866,"user_tz":-330,"elapsed":20,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["ps = PorterStemmer()"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1717923194866,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"EUvJvrZFdHn3","outputId":"774f1cef-d2c4-41a2-d97b-3030655699ba"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('jump', 'jump', 'jump')"]},"metadata":{},"execution_count":27}],"source":["ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"ppRPojcadL96","executionInfo":{"status":"ok","timestamp":1717923194867,"user_tz":-330,"elapsed":18,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["# 5_def for stemming\n","def simple_stemmer(text):\n","  ps = nltk.porter.PorterStemmer()\n","  text = ' '.join([ps.stem(word) for word in text.split()])\n","  return text"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1717923194867,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"mRBFzanZdZW5","outputId":"848e22c3-5976-4f77-8113-b14b483551eb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'my system keep crash hi crash yesterday, our crash daili'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}],"source":["simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"l_iQtRo9dflc","executionInfo":{"status":"ok","timestamp":1717923194867,"user_tz":-330,"elapsed":17,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["# 6_Lemmitization\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"M96vY_JodmMc","executionInfo":{"status":"ok","timestamp":1717923194867,"user_tz":-330,"elapsed":16,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["wnl = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2139,"status":"ok","timestamp":1717923196990,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"qY8WdKJ7dufO","outputId":"7b5ea20b-bef4-49bb-b140-a92972de3ef3"},"outputs":[{"output_type":"stream","name":"stdout","text":["car\n","men\n"]}],"source":["# lemmatize nouns with 'n'\n","print(wnl.lemmatize('cars', 'n')) # s is removed\n","print(wnl.lemmatize('men', 'n'))"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1717923196990,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"qF_2TuZ9dzjp","outputId":"98d50b0d-b44f-4881-a2e4-739638613429"},"outputs":[{"output_type":"stream","name":"stdout","text":["run\n","eat\n"]}],"source":["# lemmatize verbs with 'v'\n","print(wnl.lemmatize('running', 'v')) # extracting the verb from the verb phrase - running\n","print(wnl.lemmatize('ate', 'v')) # verb behind ate"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1717923196991,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"hE_Jt2mWd6Ol","outputId":"6d7864e6-817f-4092-d57a-779da45ff011"},"outputs":[{"output_type":"stream","name":"stdout","text":["sad\n","fancy\n"]}],"source":["# lemmatize adjectives with 'a'\n","print(wnl.lemmatize('saddest', 'a')) # orginal adjective extracted\n","print(wnl.lemmatize('fancier', 'a'))"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"baxyrdfceFaK","executionInfo":{"status":"ok","timestamp":1717923201789,"user_tz":-330,"elapsed":4803,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["# 6 - def for lemmatization\n","import spacy"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"dU0s-o1wenRG","executionInfo":{"status":"ok","timestamp":1717923202405,"user_tz":-330,"elapsed":620,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["nlp = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"KLmuD1Qbe_Ly","executionInfo":{"status":"ok","timestamp":1717923202405,"user_tz":-330,"elapsed":22,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["# 5 - def for lemmatization\n","def text_lemmatization(text):\n","  text = nlp(text)\n","  text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n","  return text"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1717923202406,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"j-2VvlfqfM3n","outputId":"9d29a997-2ed1-4f82-89ba-f968e30c55e1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'my system keep crash ! his crashed yesterday , ours crash daily'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}],"source":["text_lemmatization('My system keeps crashing! his crashed yesterday, ours crashes daily')"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1717923202406,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"K0IHQcqWj6uI","outputId":"7951bd4a-d2d5-492a-e5e9-524ce0787fb0"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":39}],"source":["# 6 - Stopwords removal\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"0EQN7MOsj8VJ","executionInfo":{"status":"ok","timestamp":1717923202406,"user_tz":-330,"elapsed":18,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["from nltk.tokenize.toktok import ToktokTokenizer"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1717923202406,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"8ec_oisFkBf1","outputId":"2ee7d101-5946-44e3-a2c7-1d1bfdcdcebb"},"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"]}],"source":["tokenizer = ToktokTokenizer()\n","stopword_list = nltk.corpus.stopwords.words('english')\n","print(stopword_list[:10])"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"93rqei12kF7n","executionInfo":{"status":"ok","timestamp":1717923202406,"user_tz":-330,"elapsed":15,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["# 6 _ def remove stop words\n","def stopword_removal(tokens):\n","  stopword_list = nltk.corpus.stopwords.words('english')\n","  filtered_tokens = [token for token in tokens if token not in stopword_list]\n","  return filtered_tokens\n","  if is_lower_case:\n","    filtered_tokens = [token for token in tokens if token not in stopword_list]\n","  else:\n","    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n","  filtered_text =''.join(filtered_tokens)\n","  return filtered_text"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1717923202407,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"t3Xil_aDkPIR","outputId":"367e3e01-1db9-42bc-ff9a-22e18f7e1954"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['T',\n"," 'h',\n"," 'e',\n"," ',',\n"," ' ',\n"," 'n',\n"," ',',\n"," ' ',\n"," 'f',\n"," ' ',\n"," 'r',\n"," 'e',\n"," ' ',\n"," 'p',\n"," 'w',\n"," 'r',\n"," ',',\n"," ' ',\n"," 'c',\n"," 'p',\n"," 'u',\n"," 'e',\n"," 'r',\n"," ' ',\n"," ' ',\n"," 'n']"]},"metadata":{},"execution_count":43}],"source":["stopword_removal(\"The, and, if are stopwords, computer is not\")"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"q2Wt8W8ETacI","executionInfo":{"status":"ok","timestamp":1717923202407,"user_tz":-330,"elapsed":13,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["corpus =(\"US unveils world's most powerful supercomputer, beats China. \"\n","\"The US has unveiled the world's most powerful supercomputer called 'Summit', \"\n","\"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n","\"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n","\"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n","\"which reportedly take up the size of two tennis courts.\")"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1717923202407,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"SMTCnKS_T2G-","outputId":"cffcc2a6-c3b9-4a21-e605-f38795fb6257"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":45}],"source":["corpus"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"EKYkBbFUrbrs","executionInfo":{"status":"ok","timestamp":1717923202407,"user_tz":-330,"elapsed":11,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["from textblob import TextBlob"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"JyyNw0ESpXem","executionInfo":{"status":"ok","timestamp":1717923202408,"user_tz":-330,"elapsed":12,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["def normalize_corpus(corpus,html_stripping=True,contraction_expansion=True,accented_char_removal=True,text_lower_case=True,\n","                     text_lemmatization=True,special_char_removal=True,stopword_removal=True):\n","  normalized_corpus = []\n","  # normalize each document in the corpus\n","  for doc in corpus:\n","    # strip HTML\n","    if html_stripping:\n","      doc = html_stripping(doc)\n","    if contraction_expansion:\n","      doc = contraction_expansion(doc)\n","    if accented_char_removal:\n","      doc = accented_char_removal(doc)\n","    if text_lower_case:\n","      doc = doc.lower()\n","      # remove extra newlines\n","      doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n","    # lemmatize text\n","    if text_lemmatization:\n","      doc = text_lemmatization(doc)\n","    # remove special characters and\\or digits\n","    if special_char_removal:\n","      # insert spaces between special characters to isolate them\n","      special_char_pattern = re.compile(r'([{.(-)!}])')\n","      doc = special_char_pattern.sub(\" \\\\1 \", doc)\n","      doc = special_char_removal(doc, remove_digits=True)\n","      # remove extra whitespace\n","      doc = re.sub(' +', ' ', doc)\n","    # remove stopwords\n","    if stopword_removal:\n","      doc = stopword_removal(doc)\n","      normalized_corpus.append(doc)\n","  return normalized_corpus\n"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4813,"status":"ok","timestamp":1717923207209,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"ndNskOIap0Td","outputId":"34224943-08d6-4fdf-b4b3-4444bff7dc8b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['u'],\n"," [],\n"," [' '],\n"," ['u'],\n"," ['n'],\n"," ['v'],\n"," ['e'],\n"," ['I'],\n"," ['l'],\n"," [],\n"," [' '],\n"," ['w'],\n"," [],\n"," ['r'],\n"," ['l'],\n"," [],\n"," [],\n"," [],\n"," [' '],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [' '],\n"," ['p'],\n"," [],\n"," ['w'],\n"," ['e'],\n"," ['r'],\n"," ['f'],\n"," ['u'],\n"," ['l'],\n"," [' '],\n"," [],\n"," ['u'],\n"," ['p'],\n"," ['e'],\n"," ['r'],\n"," ['c'],\n"," [],\n"," [],\n"," ['p'],\n"," ['u'],\n"," [],\n"," ['e'],\n"," ['r'],\n"," [],\n"," [' '],\n"," ['b'],\n"," ['e'],\n"," [],\n"," [],\n"," [],\n"," [' '],\n"," ['c'],\n"," ['h'],\n"," ['I'],\n"," ['n'],\n"," [],\n"," [' '],\n"," [' '],\n"," [],\n"," ['h'],\n"," ['e'],\n"," [' '],\n"," ['u'],\n"," [],\n"," [' '],\n"," ['h'],\n"," [],\n"," [],\n"," [' '],\n"," ['u'],\n"," ['n'],\n"," ['v'],\n"," ['e'],\n"," ['I'],\n"," ['l'],\n"," ['e'],\n"," [],\n"," [' '],\n"," [],\n"," ['h'],\n"," ['e'],\n"," [' '],\n"," ['w'],\n"," [],\n"," ['r'],\n"," ['l'],\n"," [],\n"," [],\n"," [],\n"," [' '],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [' '],\n"," ['p'],\n"," [],\n"," ['w'],\n"," ['e'],\n"," ['r'],\n"," ['f'],\n"," ['u'],\n"," ['l'],\n"," [' '],\n"," [],\n"," ['u'],\n"," ['p'],\n"," ['e'],\n"," ['r'],\n"," ['c'],\n"," [],\n"," [],\n"," ['p'],\n"," ['u'],\n"," [],\n"," ['e'],\n"," ['r'],\n"," [' '],\n"," ['c'],\n"," [],\n"," ['l'],\n"," ['l'],\n"," ['e'],\n"," [],\n"," [' '],\n"," [],\n"," [],\n"," ['u'],\n"," [],\n"," [],\n"," ['I'],\n"," [],\n"," [],\n"," [],\n"," [' '],\n"," ['b'],\n"," ['e'],\n"," [],\n"," [],\n"," ['I'],\n"," ['n'],\n"," ['g'],\n"," [' '],\n"," [],\n"," ['h'],\n"," ['e'],\n"," [' '],\n"," ['p'],\n"," ['r'],\n"," ['e'],\n"," ['v'],\n"," ['I'],\n"," [],\n"," ['u'],\n"," [],\n"," [' '],\n"," ['r'],\n"," ['e'],\n"," ['c'],\n"," [],\n"," ['r'],\n"," [],\n"," [],\n"," ['h'],\n"," [],\n"," ['l'],\n"," [],\n"," ['e'],\n"," ['r'],\n"," [' '],\n"," ['c'],\n"," ['h'],\n"," ['I'],\n"," ['n'],\n"," [],\n"," [],\n"," [],\n"," [' '],\n"," [],\n"," ['u'],\n"," ['n'],\n"," ['w'],\n"," [],\n"," [],\n"," [' '],\n"," [],\n"," [],\n"," ['I'],\n"," ['h'],\n"," ['u'],\n"," ['l'],\n"," ['I'],\n"," ['g'],\n"," ['h'],\n"," [],\n"," [' '],\n"," [' '],\n"," ['w'],\n"," ['I'],\n"," [],\n"," ['h'],\n"," [' '],\n"," [],\n"," [' '],\n"," ['p'],\n"," ['e'],\n"," [],\n"," ['k'],\n"," [' '],\n"," ['p'],\n"," ['e'],\n"," ['r'],\n"," ['f'],\n"," [],\n"," ['r'],\n"," [],\n"," [],\n"," ['n'],\n"," ['c'],\n"," ['e'],\n"," [' '],\n"," [],\n"," ['f'],\n"," [' '],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [' '],\n"," [],\n"," ['r'],\n"," ['I'],\n"," ['l'],\n"," ['l'],\n"," ['I'],\n"," [],\n"," ['n'],\n"," [' '],\n"," ['c'],\n"," [],\n"," ['l'],\n"," ['c'],\n"," ['u'],\n"," ['l'],\n"," [],\n"," [],\n"," ['I'],\n"," [],\n"," ['n'],\n"," [],\n"," [' '],\n"," ['p'],\n"," ['e'],\n"," ['r'],\n"," [' '],\n"," [],\n"," ['e'],\n"," ['c'],\n"," [],\n"," ['n'],\n"," [],\n"," [],\n"," [' '],\n"," ['I'],\n"," [],\n"," [' '],\n"," ['I'],\n"," [],\n"," [' '],\n"," [],\n"," ['v'],\n"," ['e'],\n"," ['r'],\n"," [' '],\n"," [],\n"," ['w'],\n"," ['I'],\n"," ['c'],\n"," ['e'],\n"," [' '],\n"," [],\n"," [],\n"," [' '],\n"," ['f'],\n"," [],\n"," [],\n"," [],\n"," [' '],\n"," [],\n"," [],\n"," [' '],\n"," [],\n"," ['u'],\n"," ['n'],\n"," ['w'],\n"," [],\n"," [],\n"," [' '],\n"," [],\n"," [],\n"," ['I'],\n"," ['h'],\n"," ['u'],\n"," ['l'],\n"," ['I'],\n"," ['g'],\n"," ['h'],\n"," [],\n"," [],\n"," [' '],\n"," ['w'],\n"," ['h'],\n"," ['I'],\n"," ['c'],\n"," ['h'],\n"," [' '],\n"," ['I'],\n"," [],\n"," [' '],\n"," ['c'],\n"," [],\n"," ['p'],\n"," [],\n"," ['b'],\n"," ['l'],\n"," ['e'],\n"," [' '],\n"," [],\n"," ['f'],\n"," [' '],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [' '],\n"," [],\n"," ['r'],\n"," ['I'],\n"," ['l'],\n"," ['l'],\n"," ['I'],\n"," [],\n"," ['n'],\n"," [' '],\n"," ['c'],\n"," [],\n"," ['l'],\n"," ['c'],\n"," ['u'],\n"," ['l'],\n"," [],\n"," [],\n"," ['I'],\n"," [],\n"," ['n'],\n"," [],\n"," [' '],\n"," ['p'],\n"," ['e'],\n"," ['r'],\n"," [' '],\n"," [],\n"," ['e'],\n"," ['c'],\n"," [],\n"," ['n'],\n"," [],\n"," [' '],\n"," [' '],\n"," [],\n"," ['u'],\n"," [],\n"," [],\n"," ['I'],\n"," [],\n"," [' '],\n"," ['h'],\n"," [],\n"," [],\n"," [' '],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [' '],\n"," [],\n"," ['e'],\n"," ['r'],\n"," ['v'],\n"," ['e'],\n"," ['r'],\n"," [],\n"," [],\n"," [' '],\n"," ['w'],\n"," ['h'],\n"," ['I'],\n"," ['c'],\n"," ['h'],\n"," [' '],\n"," ['r'],\n"," ['e'],\n"," ['p'],\n"," [],\n"," ['r'],\n"," [],\n"," ['e'],\n"," [],\n"," ['l'],\n"," [],\n"," [' '],\n"," [],\n"," [],\n"," ['k'],\n"," ['e'],\n"," [' '],\n"," ['u'],\n"," ['p'],\n"," [' '],\n"," [],\n"," ['h'],\n"," ['e'],\n"," [' '],\n"," [],\n"," ['I'],\n"," ['z'],\n"," ['e'],\n"," [' '],\n"," [],\n"," ['f'],\n"," [' '],\n"," [],\n"," ['w'],\n"," [],\n"," [' '],\n"," [],\n"," ['e'],\n"," ['n'],\n"," ['n'],\n"," ['I'],\n"," [],\n"," [' '],\n"," ['c'],\n"," [],\n"," ['u'],\n"," ['r'],\n"," [],\n"," [],\n"," [' ']]"]},"metadata":{},"execution_count":48}],"source":["normalize_corpus(corpus,html_stripping=html_stripping,contraction_expansion=TextBlob,accented_char_removal=accented_char_removal,text_lower_case=True,text_lemmatization=text_lemmatization,special_char_removal=special_char_removal,stopword_removal=stopword_removal)"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"bw6f_ZI_uhVv","executionInfo":{"status":"ok","timestamp":1717923207209,"user_tz":-330,"elapsed":18,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["def normalize_corpus(corpus):\n","  normalized_corpus = []\n","  for text in corpus:\n","    # 1\n","    text=html_stripping(text)\n","    # 2\n","    text=special_char_removal(text)\n","    # 3\n","    text=contraction_expansion(text,contraction_mapping)\n","    # 4\n","    # text=simple_stemmer(text)\n","    # 5\n","    text=accented_char_removal(text)\n","    # 6\n","    text=text_lemmatization(text)\n","    # 7\n","    text=stopword_removal(text)\n","    normalized_corpus.append(text)\n","    #if tokenize:\n","    #  text = tokenize_text(text)\n","    # normalized_corpus.append(text)\n","  return normalized_corpus"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7796,"status":"ok","timestamp":1717923214988,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"doclljwKxbBx","outputId":"4f34fe32-e431-4c02-8bb5-3b1406377290"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e'],\n"," ['n', 'n', 'e']]"]},"metadata":{},"execution_count":50}],"source":["normalize_corpus(corpus)"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15991,"status":"ok","timestamp":1717923230963,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"quVqkLbir17c","outputId":"5f73630d-397a-444d-e252-8026f08904ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting text_normalizer\n","  Downloading text-normalizer-0.1.3.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from text_normalizer) (2.0.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer) (2024.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer) (1.25.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->text_normalizer) (1.16.0)\n","Building wheels for collected packages: text_normalizer\n","  Building wheel for text_normalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for text_normalizer: filename=text_normalizer-0.1.3-cp310-cp310-linux_x86_64.whl size=247444 sha256=79f76f21e8e4a8a650ad1845c5e0ddd43186c73c55840c3ecd4de5e5e3b723d2\n","  Stored in directory: /root/.cache/pip/wheels/fa/9f/80/4a4e7d2d6f6fc35b19993353c2c8f1f7ac48ac29c826d2e676\n","Successfully built text_normalizer\n","Installing collected packages: text_normalizer\n","Successfully installed text_normalizer-0.1.3\n"]}],"source":["# Install Text Normalizer for extracting the newsgoup data set\n","!pip install text_normalizer"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"DPt2BLsNlQbf","executionInfo":{"status":"ok","timestamp":1717923230963,"user_tz":-330,"elapsed":18,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["import text_normalizer"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"QPUWf7fDMWhL","executionInfo":{"status":"ok","timestamp":1717923230964,"user_tz":-330,"elapsed":18,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["from sklearn.datasets import fetch_20newsgroups"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"Pfu510qpMgJn","executionInfo":{"status":"ok","timestamp":1717923231787,"user_tz":-330,"elapsed":841,"user":{"displayName":"professor","userId":"02563211122263964727"}}},"outputs":[],"source":["# Create objects for each package\n","import numpy as np\n","#import text_normalizer as tn\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"code","source":["import time\n","from sklearn.datasets import fetch_20newsgroups\n","\n","# Wait for 60 seconds before trying again\n","time.sleep(60)\n","\n","# Fetch data\n","data = fetch_20newsgroups(subset='all')\n","data = fetch_20newsgroups(subset='all', shuffle=True, remove=('headers', 'footers', 'quotes'))\n","data_labels_map = dict(enumerate(data.target_names))\n","\n","print(\"Dataset fetched successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"O1v_S5jlNiT0","executionInfo":{"status":"error","timestamp":1717923427457,"user_tz":-330,"elapsed":60833,"user":{"displayName":"professor","userId":"02563211122263964727"}},"outputId":"01104b21-2a67-4883-94a0-a40d0f0651d8"},"execution_count":56,"outputs":[{"output_type":"error","ename":"HTTPError","evalue":"HTTP Error 403: Forbidden","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-6197c6542e35>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Fetch data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'headers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'footers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quotes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdata_labels_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_twenty_newsgroups.py\u001b[0m in \u001b[0;36mfetch_20newsgroups\u001b[0;34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload_if_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading 20news dataset. This may take a few minutes.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             cache = _download_20newsgroups(\n\u001b[0m\u001b[1;32m    270\u001b[0m                 \u001b[0mtarget_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtwenty_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_twenty_newsgroups.py\u001b[0m in \u001b[0;36m_download_20newsgroups\u001b[0;34m(target_dir, cache_path)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading dataset from %s (14 MB)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mARCHIVE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0marchive_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fetch_remote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARCHIVE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Decompressing %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_base.py\u001b[0m in \u001b[0;36m_fetch_remote\u001b[0;34m(remote, dirname)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdirname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m     \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m     \u001b[0mchecksum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sha256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecksum\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mchecksum\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"]}]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1717923231788,"user":{"displayName":"professor","userId":"02563211122263964727"},"user_tz":-330},"id":"X-82brBxgsL9"},"outputs":[],"source":["# building the dataframe for the data extracted from newgroups\n","# Create a corpus of newsgroup sentences\n","corpus=data.data\n","target_labels=data.target\n","target_names = [data_labels_map[label] for label in data.target]\n","data_df = pd.DataFrame({'Article': corpus, 'Target Label': target_labels,'Target Name': target_names})\n","print(data_df.shape)\n","data_df.head(30)"]},{"cell_type":"markdown","metadata":{"id":"pfMDTvZbkH1f"},"source":["From this dataset, we can see that each document has some textual content and the\n","label can be denoted by a specific number, which maps to a newsgroup category name"]}],"metadata":{"colab":{"provenance":[{"file_id":"1iSMNNBUrGW_8WNlJLw8SUN3IStQEpuW_","timestamp":1708500891155}],"authorship_tag":"ABX9TyODcrku7KyBe4XsRBbqE/lY"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}