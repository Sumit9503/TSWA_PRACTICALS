{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"1uBpMsZgYx5QqcMQQyMAyGR_tZzjbGcqX","authorship_tag":"ABX9TyPO7JxoOGSm2P/zrE3+ElWN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# TSWA_PRACTICAL_7_TEXT_NORMALIZATION\n","!pip install nltk"],"metadata":{"id":"RYMjQ9auOaKE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import string\n","import nltk"],"metadata":{"id":"df4Ks_cROvCR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.tokenize import word_tokenize , sent_tokenize\n","from nltk.corpus import wordnet"],"metadata":{"id":"VwdY7BhyO0Qh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')"],"metadata":{"id":"tPrk2umGOk05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install contractions"],"metadata":{"id":"lUfTOQD8PPnu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LqW_RlvuOPdr"},"outputs":[],"source":["import contractions"]},{"cell_type":"code","source":["def contraction_remover(text):\n","    expanded_words = []\n","    for word in text.split():\n","    # using contractions.fix to expand the shortened words\n","        expanded_words.append(contractions.fix(word))\n","    expanded_text = ' '.join(expanded_words)\n"],"metadata":{"id":"H6Ra0Dn8OX5G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalize_corpus(df):\n","    # Remove HTML tags\n","    df['Preprocess_Article'] = df['Article'].apply(lambda x: re.sub(r'<.*?>', '', x))\n","\n","    # Convert to lowercase\n","    df['Preprocess_Article'] = df['Preprocess_Article'].str.lower()\n","\n","    # Remove URLs\n","    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'http\\S+|www\\.\\S+', '', x))\n","\n","    # Remove email addresses\n","    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\S+@\\S+', '', x))\n","\n","    # Remove phone numbers\n","    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\d{10}', '', x))\n","\n","    # Handle negation\n","    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\bnot\\b(\\w+)', r'not_\\1', x))\n","\n","    # Remove special characters and punctuation\n","    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n","    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n","\n","    # Remove numeric tokens\n","    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\b\\d+\\b', '', x))\n","\n","    # Tokenization\n","    df['tokens'] = df['Preprocess_Article'].apply(word_tokenize)\n","\n","    # Remove stopwords\n","    stop_words = set(stopwords.words('english'))\n","    df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n","\n","    # Stemming\n","    stemmer = PorterStemmer()\n","    df['tokens'] = df['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n","\n","    # Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word,get_wordnet_pos(word))for word in x])\n","\n","     # Convert tokens into a single string\n","    df['Clean Article'] = df['tokens'].apply(lambda x: ' '.join(x))\n","\n","    return df"],"metadata":{"id":"T9g_2zomPgcE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Helper function to map POS tag to WordNet POS tag\n","def get_wordnet_pos(word):\n","  tag = nltk.pos_tag([word])[0][1][0].upper()\n","  tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n","  return tag_dict.get(tag, wordnet.NOUN)"],"metadata":{"id":"9F1nea9cZMn7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.datasets import fetch_20newsgroups"],"metadata":{"id":"KVqOUNZEQSEJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fetch data\n","data=fetch_20newsgroups(subset='all')\n","data = fetch_20newsgroups(subset='all', shuffle=True,remove=('headers', 'footers', 'quotes'))\n","data_labels_map = dict(enumerate(data.target_names))"],"metadata":{"id":"Oe7EwsNDQA8Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create objects for each package\n","import numpy as np\n","#import text_normalizer as tn\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"metadata":{"id":"o9rUtI1CQemB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# building the dataframe for the data extracted from newgroups\n","# Create a corpus of newsgroup sentences and create the data frame\n","corpus=data.data\n","target_labels=data.target\n","target_names = [data_labels_map[label] for label in data.target]\n","data_df = pd.DataFrame({'Article': corpus, 'Target Label': target_labels,'Target Name': target_names})\n","print(data_df.shape)\n","data_df.head(10)"],"metadata":{"id":"B69zflU-QamV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df.info()"],"metadata":{"id":"5YxplfbTyA__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["normalize_corpus(data_df)"],"metadata":{"id":"B89rX5QgSqor"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df.info()"],"metadata":{"id":"VP0wnustzRHQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# view sample data\n","data_df = data_df[['Article', 'Clean Article', 'Target Label', 'Target Name']]\n","data_df.head(10)"],"metadata":{"id":"pToY8qSW7t6P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove any unwanted characters\n","data_df = data_df.replace(r'^(\\s?)+$', np.nan, regex=True)\n","data_df.info()"],"metadata":{"id":"dd_NUmze79F8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df = data_df.dropna().reset_index(drop=True)\n","data_df.info()"],"metadata":{"id":"7JaFOQGs8KwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a csv file of the cleaned doucment so that it can be reused\n","data_df.to_csv('clean_newsgroups.csv', index=False)"],"metadata":{"id":"9LXddnh68RjV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data - training and testing\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"zIReOtmt9B1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_corpus, test_corpus, train_label_nums, test_label_nums, train_label_names, test_label_names = train_test_split(np.array(data_df['Clean Article']), np.array(data_df['Target Label']),\n","np.array(data_df['Target Name']),test_size=0.33, random_state=42)"],"metadata":{"id":"AfjJm8xW9GPi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_corpus.shape, test_corpus.shape"],"metadata":{"id":"LV37Lv3l9Veu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the dictionary for train and test data\n","from collections import Counter"],"metadata":{"id":"zGSTA2EH9fTZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trd = dict(Counter(train_label_names))\n","tsd = dict(Counter(test_label_names))"],"metadata":{"id":"pchrhgAX9ifP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trd"],"metadata":{"id":"CswxNlvAQ3qN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tsd"],"metadata":{"id":"N_Sa_2YURE1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(pd.DataFrame([[key, trd[key], tsd[key]] for key in trd],columns=['Target Label', 'Train Count', 'Test Count'])\n",".sort_values(by=['Train Count', 'Test Count'],ascending=False))"],"metadata":{"id":"RzvlBk8V9mxL"},"execution_count":null,"outputs":[]}]}